"""
Dynamic Query Mechanism for Adaptive Object Detection
åŠ¨æ€æŸ¥è¯¢æœºåˆ¶ï¼šåŸºäºå¯å­¦ä¹ è¾¹ç•Œå’Œè½¯åŒºé—´åˆ†é…
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math

def safe_prob_normalize(probs):
    """å®‰å…¨æ¦‚ç‡å½’ä¸€åŒ–"""
    denom = probs.sum(dim=1, keepdim=True)
    denom = denom.clamp(min=1e-6) # é˜²æ­¢é™¤ä»¥0
    return probs / denom
class LearnableBoundaryPredictor(nn.Module):
    """
    å¯å­¦ä¹ è¾¹ç•Œé¢„æµ‹å™¨
    åŠŸèƒ½ï¼šæ ¹æ®å¯†åº¦ç‰¹å¾é¢„æµ‹è¾¹ç•Œ[b1, b2, b3]ï¼Œå°†ç›®æ ‡æ•°é‡åˆ†ä¸º4ä¸ªåŒºé—´
    """

    def __init__(self, feature_dim=256, num_boundaries=3, max_objects=1500,
                 initial_smoothness=1.0):
        """
        å‚æ•°:
            feature_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            num_boundaries: è¾¹ç•Œæ•°é‡ï¼ˆé»˜è®¤3ä¸ªï¼Œåˆ’åˆ†4ä¸ªåŒºé—´ï¼‰
            max_objects: æœ€å¤§ç›®æ ‡æ•°é‡
            initial_smoothness: åˆå§‹å¹³æ»‘ç³»æ•°r
        """
        super().__init__()
        self.num_boundaries = num_boundaries
        self.max_objects = max_objects

        # å…¨å±€ç‰¹å¾æå–
        self.global_pool_avg = nn.AdaptiveAvgPool2d(1)
        self.global_pool_max = nn.AdaptiveMaxPool2d(1)

        # è¾¹ç•Œé¢„æµ‹ç½‘ç»œï¼šèåˆavgå’Œmax poolingç‰¹å¾
        self.boundary_predictor = nn.Sequential(
            nn.Linear(feature_dim * 2, feature_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            nn.Linear(feature_dim, feature_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(feature_dim // 2, num_boundaries)
        )

        # å¹³æ»‘ç³»æ•°rï¼ˆå¯å­¦ä¹ ï¼‰
        self.register_buffer('smoothness', torch.tensor(initial_smoothness))

        # åˆå§‹åŒ–è¾¹ç•Œé¢„æµ‹å™¨ï¼Œä½¿è¾“å‡ºæ¥è¿‘[0.25, 0.5, 0.75] * max_objects
        nn.init.constant_(self.boundary_predictor[-1].bias, 0.0)
        with torch.no_grad():
            # è®©åˆå§‹è¾¹ç•Œå¤§è‡´è½åœ¨å‡åŒ€åˆ†å¸ƒ
            self.boundary_predictor[-1].weight.data *= 0.01

    def forward(self, density_feature):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥:
            density_feature: (BS, C, H, W) - CCMè¾“å‡ºçš„å¯†åº¦ç‰¹å¾
        è¾“å‡º:
            boundaries: (BS, num_boundaries) - å­¦ä¹ åˆ°çš„è¾¹ç•Œå€¼ [b1, b2, b3]
            raw_boundaries: (BS, num_boundaries) - åŸå§‹è¾¹ç•Œå€¼ [t1, t2, t3]
        """
        # ğŸ”¥ è¾“å…¥æ£€æŸ¥
        if torch.isnan(density_feature).any() or torch.isinf(density_feature).any():
            # print("  Warning: density_feature has NaN/Inf, applying fix") # å¯é€‰ï¼šæ³¨é‡Šæ‰é¿å…åˆ·å±
            density_feature = torch.nan_to_num(density_feature, nan=0.0, posinf=1.0, neginf=0.0)
            density_feature = density_feature.clamp(min=-10.0, max=10.0)

        # 1. å…¨å±€ç‰¹å¾æå–
        feat_avg = self.global_pool_avg(density_feature).flatten(1)  # (BS, C)
        feat_max = self.global_pool_max(density_feature).flatten(1)  # (BS, C)

        # ğŸ”¥ ç‰¹å¾è£å‰ª
        feat_avg = feat_avg.clamp(min=-10.0, max=10.0)
        feat_max = feat_max.clamp(min=-10.0, max=10.0)

        global_feat = torch.cat([feat_avg, feat_max], dim=1)  # (BS, 2C)

        # 2. é¢„æµ‹åŸå§‹è¾¹ç•Œ
        raw_boundaries = self.boundary_predictor(global_feat)  # (BS, 3)
        raw_boundaries = raw_boundaries.clamp(min=-10.0, max=10.0)

        # 3. ç´¯åŠ Softplus
        softplus_values = F.softplus(raw_boundaries)
        softplus_values = softplus_values.clamp(min=1e-4, max=self.max_objects / 4)
        boundaries = torch.cumsum(softplus_values, dim=1)

        # 4. å½’ä¸€åŒ–åˆ° [0, max_objects] èŒƒå›´
        boundaries = boundaries.clamp(min=1.0, max=self.max_objects * 0.9)

        # ==================== ğŸ”¥ ä¿®å¤å¼€å§‹ ====================
        # ä¿®å¤è¯´æ˜ï¼šæ¶ˆé™¤ boundaries[:, i] = ... çš„åŸä½ä¿®æ”¹
        # æ”¹ä¸ºä½¿ç”¨åˆ—è¡¨æ”¶é›†æ¯ä¸€åˆ—ï¼Œæœ€å stack

        boundaries_list = []
        # ç¬¬ä¸€ä¸ªè¾¹ç•Œ b1 ç›´æ¥å–å€¼
        boundaries_list.append(boundaries[:, 0])

        min_gap = 10.0
        for i in range(1, boundaries.shape[1]):
            # è·å–ä¸Šä¸€ä¸ªå·²å¤„ç†çš„è¾¹ç•Œï¼ˆæ¥è‡ªåˆ—è¡¨ï¼Œè€Œä¸æ˜¯åŸå¼ é‡ï¼‰
            prev_b = boundaries_list[-1]
            # è·å–å½“å‰é¢„æµ‹çš„åŸå§‹è¾¹ç•Œ
            curr_b = boundaries[:, i]

            # è®¡ç®—æ–°çš„å½“å‰è¾¹ç•Œï¼šmax(å½“å‰å€¼, ä¸Šä¸€ä¸ªå€¼ + é—´éš”)
            # è¿™ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡ new_bï¼Œè€Œä¸æ˜¯ä¿®æ”¹åŸå¼ é‡
            new_b = torch.max(curr_b, prev_b + min_gap)
            boundaries_list.append(new_b)

        # é‡æ–°å †å å› (BS, 3)
        boundaries = torch.stack(boundaries_list, dim=1)
        # ==================== ğŸ”¥ ä¿®å¤ç»“æŸ ====================

        return boundaries, raw_boundaries

    def compute_interval_probabilities(self, boundaries, real_count):
        """
        è®¡ç®—ç›®æ ‡æ•°é‡real_countå±äºå„åŒºé—´çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆè½¯åŒºé—´åˆ†é…ï¼‰
        è¾“å…¥:
            boundaries: (BS, 3) - è¾¹ç•Œå€¼ [b1, b2, b3]
            real_count: (BS,) - çœŸå®ç›®æ ‡æ•°é‡
        è¾“å‡º:
            probs: (BS, 4) - å››ä¸ªåŒºé—´çš„æ¦‚ç‡åˆ†å¸ƒ
        """

        b1, b2, b3 = boundaries[:, 0], boundaries[:, 1], boundaries[:, 2]
        N = real_count.float().unsqueeze(1)
        r = self.smoothness.clamp(min=0.1, max=5.0)  # é™åˆ¶èŒƒå›´

        # ä½¿ç”¨ tanh æ›¿ä»£ sigmoidï¼Œæ•°å€¼æ›´ç¨³å®š
        def soft_interval(x, lower, upper, r):
            """è½¯åŒºé—´æŒ‡ç¤ºå‡½æ•°"""
            left = torch.tanh((x - lower) / r)
            right = torch.tanh((upper - x) / r)
            return ((left + 1) * (right + 1) / 4).clamp(0, 1)

        # è®¡ç®—å››ä¸ªåŒºé—´çš„æ¦‚ç‡
        p1 = soft_interval(N, 0, b1, r)
        p2 = soft_interval(N, b1, b2, r)
        p3 = soft_interval(N, b2, b3, r)
        p4 = soft_interval(N, b3, self.max_objects, r)

        probs = torch.cat([p1, p2, p3, p4], dim=1)
        probs = safe_prob_normalize(probs)

        return probs

    def get_query_number(self, boundaries, predicted_count, query_levels):
        """
        æ¨ç†é˜¶æ®µï¼šæ ¹æ®é¢„æµ‹çš„ç›®æ ‡æ•°é‡å’Œè¾¹ç•Œï¼Œç¡®å®šæŸ¥è¯¢æ•°é‡
        è¾“å…¥:
            boundaries: (BS, 3) - è¾¹ç•Œå€¼
            predicted_count: (BS,) - é¢„æµ‹çš„ç›®æ ‡æ•°é‡
            query_levels: list[int] - å››ä¸ªæŸ¥è¯¢ç­‰çº§ï¼Œå¦‚[500, 1000, 1500, 2000]
        è¾“å‡º:
            num_queries: (BS,) - æ¯ä¸ªæ ·æœ¬çš„æŸ¥è¯¢æ•°é‡
        """
        BS = boundaries.shape[0]
        device = boundaries.device

        b1, b2, b3 = boundaries[:, 0], boundaries[:, 1], boundaries[:, 2]
        N = predicted_count.float()

        # æ ¹æ®Nè½åœ¨å“ªä¸ªåŒºé—´ï¼Œé€‰æ‹©å¯¹åº”çš„æŸ¥è¯¢æ•°é‡
        num_queries = torch.zeros(BS, dtype=torch.long, device=device)

        # åŒºé—´1: N <= b1
        mask1 = N <= b1
        num_queries[mask1] = query_levels[0]

        # åŒºé—´2: b1 < N <= b2
        mask2 = (N > b1) & (N <= b2)
        num_queries[mask2] = query_levels[1]

        # åŒºé—´3: b2 < N <= b3
        mask3 = (N > b2) & (N <= b3)
        num_queries[mask3] = query_levels[2]

        # åŒºé—´4: N > b3
        mask4 = N > b3
        num_queries[mask4] = query_levels[3]

        return num_queries

    def update_smoothness(self, epoch, total_epochs, min_smoothness=0.1):
        """
        åŠ¨æ€è°ƒæ•´å¹³æ»‘ç³»æ•°rï¼šä»initial_smoothnessé€æ¸è¡°å‡åˆ°min_smoothness
        """
        old_val = self.smoothness.item()
        decay_rate = (old_val - min_smoothness) / total_epochs
        new_smoothness = max(old_val - decay_rate, min_smoothness)

        # ğŸ”¥ é™åˆ¶å•æ¬¡å˜åŒ–å¹…åº¦
        max_change = 0.5
        if abs(new_smoothness - old_val) > max_change:
            new_smoothness = old_val + max_change * (1 if new_smoothness > old_val else -1)

        self.smoothness.fill_(new_smoothness)


class QualityAwareQueryInitializer(nn.Module):
    """
    è´¨é‡æ„ŸçŸ¥çš„æŸ¥è¯¢ä½ç½®åˆå§‹åŒ–
    åŠŸèƒ½ï¼šé€šè¿‡åŒé‡æ³¨æ„åŠ›æœºåˆ¶ï¼ˆé€šé“+ç©ºé—´ï¼‰é¢„æµ‹é«˜è´¨é‡çš„æŸ¥è¯¢åˆå§‹ä½ç½®
    """

    def __init__(self, feature_dim=256, num_heads=8, max_queries=1500):
        super().__init__()
        self.feature_dim = feature_dim
        self.max_queries = max_queries

        # é€šé“æ³¨æ„åŠ›
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(feature_dim, feature_dim // 16, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim // 16, feature_dim, 1),
            nn.Sigmoid()
        )

        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3),
            nn.Sigmoid()
        )

        # ä½ç½®è´¨é‡é¢„æµ‹ï¼ˆç”¨äºç­›é€‰Top-Kä½ç½®ï¼‰
        self.quality_predictor = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, 1, 1),
            nn.Sigmoid()
        )

        # åæ ‡å›å½’å¤´ï¼ˆé¢„æµ‹4ç»´åæ ‡ cx, cy, w, hï¼‰
        self.coord_regressor = nn.Sequential(
            nn.Conv2d(feature_dim, feature_dim, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_dim, 4, 1),
            nn.Sigmoid()  # å½’ä¸€åŒ–åˆ°[0,1]
        )

    def forward(self, encoder_features, num_queries):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥:
            encoder_features: (BS, C, H, W) - encoderè¾“å‡ºçš„ç‰¹å¾å›¾
            num_queries: (BS,) æˆ– int - æ¯ä¸ªæ ·æœ¬éœ€è¦çš„æŸ¥è¯¢æ•°é‡
        è¾“å‡º:
            reference_points: (BS, num_queries, 4) - åˆå§‹å‚è€ƒç‚¹ (cx, cy, w, h)
            quality_scores: (BS, num_queries) - è´¨é‡åˆ†æ•°
        """
        BS, C, H, W = encoder_features.shape
        device = encoder_features.device
        #  è¾“å…¥æ£€æŸ¥
        if torch.isnan(encoder_features).any() or torch.isinf(encoder_features).any():
            encoder_features = torch.nan_to_num(encoder_features, nan=0.0).clamp(-10.0, 10.0)

        # å¦‚æœnum_queriesæ˜¯æ ‡é‡ï¼Œè½¬ä¸ºtensor
        if isinstance(num_queries, int):
            num_queries = torch.full((BS,), num_queries, dtype=torch.long, device=device)

        # å¤„ç†ç©ºç‰¹å¾å›¾
        if H == 0 or W == 0:
            print(f"  Warning: Empty feature map (H={H}, W={W}), using random initialization")
            max_K = num_queries.max().item()
            reference_points = torch.rand(BS, max_K, 4, device=device) * 0.5 + 0.25
            quality_scores = torch.ones(BS, max_K, device=device) * 0.5
            return reference_points, quality_scores

        # 1. é€šé“æ³¨æ„åŠ›
        channel_attn = self.channel_attention(encoder_features)
        feat_ca = encoder_features * channel_attn

        # 2. ç©ºé—´æ³¨æ„åŠ›
        feat_max = torch.max(feat_ca, dim=1, keepdim=True)[0]
        feat_avg = torch.mean(feat_ca, dim=1, keepdim=True)
        spatial_input = torch.cat([feat_max, feat_avg], dim=1)
        spatial_attn = self.spatial_attention(spatial_input)
        feat_refined = feat_ca * spatial_attn

        # 3. è´¨é‡é¢„æµ‹
        quality_map = self.quality_predictor(feat_refined).squeeze(1)  # (BS, H, W)
        # é˜²æ­¢è´¨é‡å›¾å¼‚å¸¸
        quality_map = torch.nan_to_num(quality_map, nan=0.5, posinf=1.0, neginf=0.0)
        quality_map = quality_map.clamp(min=0.0, max=1.0)

        # 4. åæ ‡å›å½’
        coords_map = self.coord_regressor(feat_refined).permute(0, 2, 3, 1)
        coords_map = torch.nan_to_num(coords_map, nan=0.5)
        # ä¿®æ”¹ï¼šé™åˆ¶åæ ‡èŒƒå›´ï¼Œç•™å‡ºepsä½™é‡ï¼Œé˜²æ­¢inverse_sigmoidçˆ†ç‚¸
        coords_map = coords_map.clamp(min=0.05, max=0.95)

        # 5. Top-Ké€‰æ‹©ï¼ˆä¸ºæ¯ä¸ªæ ·æœ¬é€‰æ‹©ä¸åŒæ•°é‡çš„æŸ¥è¯¢ï¼‰
        max_K = num_queries.max().item()
        quality_flat = quality_map.flatten(1)  # (BS, H*W)

        # é€‰æ‹©Top-Kä½ç½®
        # topk_values, topk_indices = torch.topk(quality_flat, max_K, dim=1)  # (BS, max_K)
        # æ ¸å¿ƒä¿®æ”¹ï¼šç¡®ä¿ k ä¸è¶…è¿‡å¼ é‡å®é™…å¤§å°
        actual_k = min(max_K, quality_flat.shape[1])
        if actual_k == 0:
            # å¤„ç†ç‰¹å¾å›¾ H*W=0 çš„æç«¯æƒ…å†µï¼Œè¿”å›ç©ºå¼ é‡
            topk_values = torch.empty((BS, max_K), dtype=torch.float32, device=device)
            topk_indices = torch.empty((BS, max_K), dtype=torch.long, device=device)
        else:
            # é€‰æ‹©Top-Kä½ç½®
            topk_values_actual, topk_indices_actual = torch.topk(quality_flat, actual_k, dim=1)

            # å¦‚æœå®é™…é€‰æ‹©çš„ < max_Kï¼Œç”¨0å¡«å……è‡³ max_K
            topk_values = torch.zeros(BS, max_K, device=device)
            topk_indices = torch.zeros(BS, max_K, dtype=torch.long, device=device)
            topk_values[:, :actual_k] = topk_values_actual
            topk_indices[:, :actual_k] = topk_indices_actual

        # 6. æå–å¯¹åº”ä½ç½®çš„åæ ‡
        # å°†1Dç´¢å¼•è½¬æ¢ä¸º2Dåæ ‡
        topk_y = topk_indices // W
        topk_x = topk_indices % W

        # æ”¶é›†åæ ‡
        reference_points_list = []
        quality_scores_list = []

        for b in range(BS):
            K = num_queries[b].item()
            # ç¡®ä¿ K ä¸è¶…è¿‡ actual_kï¼Œé˜²æ­¢è¶Šç•Œ
            K_safe = min(K, actual_k)

            if K_safe > 0:
                coords_selected = coords_map[b, topk_y[b, :K_safe], topk_x[b, :K_safe], :]
                quality_selected = topk_values[b, :K_safe]
            else:
                coords_selected = torch.empty((0, 4), device=device)
                quality_selected = torch.empty((0,), device=device)

            reference_points_list.append(coords_selected)
            quality_scores_list.append(quality_selected)

        # 7. Paddingåˆ°ç»Ÿä¸€é•¿åº¦ï¼ˆmax_Kï¼‰
        # ğŸ”¥ğŸ”¥ ä¿®æ”¹ï¼šåˆå§‹åŒ–ä¸º 0.5 (å›¾åƒä¸­å¿ƒ)ï¼Œè€Œä¸æ˜¯ 0.0
        # 0.0 ç»è¿‡ inverse_sigmoid ä¼šå˜æˆè´Ÿæ— ç©·æˆ–æå¤§è´Ÿæ•°ï¼Œå¯¼è‡´ attention é‡‡æ ·è¶Šç•Œå’Œ NaN æ¢¯åº¦
        reference_points = torch.full((BS, max_K, 4), 0.5, device=device)
        # ä¸º padding çš„ä½ç½®è®¾ç½®åˆç†çš„é»˜è®¤æ¡†ï¼šä¸­å¿ƒä½ç½®ï¼Œå°å°ºå¯¸
        reference_points[..., 2:] = 0.1  # w, h = 0.1

        quality_scores = torch.zeros(BS, max_K, device=device)

        for b in range(BS):
            K = num_queries[b].item()
            K_safe = min(K, actual_k)
            if K_safe > 0:
                reference_points[b, :K_safe] = reference_points_list[b]
                quality_scores[b, :K_safe] = quality_scores_list[b]

        return reference_points, quality_scores


class DynamicQueryModule(nn.Module):
    """
    åŠ¨æ€æŸ¥è¯¢æœºåˆ¶æ€»æ¨¡å—
    æ•´åˆï¼šè¾¹ç•Œé¢„æµ‹ + æŸ¥è¯¢åˆå§‹åŒ–
    """

    def __init__(self,
                 feature_dim=256,
                 num_boundaries=3,
                 max_objects=1500,
                 query_levels=None,
                 initial_smoothness=1.0):
        super().__init__()

        if query_levels is None:
            query_levels = [300, 500, 900, 1500]
        self.query_levels = query_levels

        # è¾¹ç•Œé¢„æµ‹å™¨
        self.boundary_predictor = LearnableBoundaryPredictor(
            feature_dim=feature_dim,
            num_boundaries=num_boundaries,
            max_objects=max_objects,
            initial_smoothness=initial_smoothness
        )

        # æŸ¥è¯¢åˆå§‹åŒ–å™¨
        self.query_initializer = QualityAwareQueryInitializer(
            feature_dim=feature_dim,
            max_queries=max(query_levels)
        )

    def forward(self, density_feature, encoder_feature, real_counts=None, training=True):
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥:
            density_feature: (BS, C, H, W) - CCMè¾“å‡ºçš„å¯†åº¦ç‰¹å¾
            encoder_feature: (BS, C, H, W) - Encoderè¾“å‡ºçš„ç‰¹å¾
            real_counts: (BS,) - çœŸå®ç›®æ ‡æ•°é‡ï¼ˆä»…è®­ç»ƒæ—¶éœ€è¦ï¼‰
            training: bool - æ˜¯å¦è®­ç»ƒæ¨¡å¼
        è¾“å‡º:
            outputs: dict - åŒ…å«è¾¹ç•Œã€æŸ¥è¯¢æ•°é‡ã€å‚è€ƒç‚¹ç­‰ä¿¡æ¯
        """
        BS = density_feature.shape[0]
        device = density_feature.device

        # 1. é¢„æµ‹è¾¹ç•Œ
        boundaries, raw_boundaries = self.boundary_predictor(density_feature)

        outputs = {
            'boundaries': boundaries,  # (BS, 3)
            'raw_boundaries': raw_boundaries,  # (BS, 3)
        }

        if training and real_counts is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—è½¯åŒºé—´æ¦‚ç‡
            interval_probs = self.boundary_predictor.compute_interval_probabilities(
                boundaries, real_counts
            )
            outputs['interval_probs'] = interval_probs  # (BS, 4)

            # ä½¿ç”¨çœŸå®ç›®æ ‡æ•°é‡ç¡®å®šæŸ¥è¯¢æ•°é‡ï¼ˆç”¨äºè®­ç»ƒç¨³å®šæ€§ï¼‰
            # æˆ–è€…ä½¿ç”¨æœŸæœ›æŸ¥è¯¢æ•°é‡
            num_queries = self._get_expected_query_number(interval_probs)

        else:
            # æ¨ç†æ¨¡å¼ï¼šæ ¹æ®é¢„æµ‹çš„ç›®æ ‡æ•°é‡ç¡®å®šæŸ¥è¯¢æ•°é‡
            # è¿™é‡Œéœ€è¦ä»density_featureä¼°è®¡ç›®æ ‡æ•°é‡
            predicted_count = self._estimate_object_count(density_feature)
            num_queries = self.boundary_predictor.get_query_number(
                boundaries, predicted_count, self.query_levels
            )
            outputs['predicted_count'] = predicted_count

        outputs['num_queries'] = num_queries  # (BS,)

        # 2. åˆå§‹åŒ–æŸ¥è¯¢ä½ç½®
        try:
            reference_points, quality_scores = self.query_initializer(
                encoder_feature, num_queries
            )
            outputs['reference_points'] = reference_points
            outputs['quality_scores'] = quality_scores
        except Exception as e:
            print(f"Query initialization error: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼
            max_K = max(self.query_levels)
            outputs['reference_points'] = torch.rand(
                BS, max_K, 4, device=device
            ) * 0.5 + 0.25
            outputs['quality_scores'] = torch.ones(
                BS, max_K, device=device
            ) * 0.5

        return outputs

    def _get_expected_query_number(self, interval_probs):
        """
        æ ¹æ®è½¯åŒºé—´æ¦‚ç‡è®¡ç®—æœŸæœ›æŸ¥è¯¢æ•°é‡
        """
        BS = interval_probs.shape[0]
        device = interval_probs.device

        query_levels_tensor = torch.tensor(
            self.query_levels, dtype=torch.float32, device=device
        )

        # æœŸæœ›æŸ¥è¯¢æ•°é‡ = sum(p_i * K_i)
        expected_queries = (interval_probs * query_levels_tensor).sum(dim=1)

        # å–æœ€æ¥è¿‘çš„æŸ¥è¯¢ç­‰çº§
        num_queries = torch.zeros(BS, dtype=torch.long, device=device)
        for b in range(BS):
            diffs = torch.abs(query_levels_tensor - expected_queries[b])
            num_queries[b] = self.query_levels[torch.argmin(diffs)]

        return num_queries

    def _estimate_object_count(self, density_feature):
        """
        ä»å¯†åº¦ç‰¹å¾ä¼°è®¡ç›®æ ‡æ•°é‡ï¼ˆç®€å•æ±‚å’Œï¼‰
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®CCMçš„è¾“å‡ºè¿›è¡Œä¼°è®¡
        # ç®€åŒ–ç‰ˆæœ¬ï¼šå¯¹å¯†åº¦å›¾æ±‚å’Œ
        count = density_feature.sum(dim=(1, 2, 3))
        return count.long()

    def update_smoothness(self, epoch, total_epochs):
        """æ›´æ–°å¹³æ»‘ç³»æ•°"""
        self.boundary_predictor.update_smoothness(epoch, total_epochs)


def build_dynamic_query_module(args):
    """å·¥å‚å‡½æ•°ï¼šæ„å»ºåŠ¨æ€æŸ¥è¯¢æ¨¡å—"""
    return DynamicQueryModule(
        feature_dim=args.hidden_dim,
        num_boundaries=getattr(args, 'num_boundaries', 3),
        max_objects=getattr(args, 'max_objects', 1500),
        query_levels=getattr(args, 'dynamic_query_levels', [300, 500, 900, 1500]),
        initial_smoothness=getattr(args, 'initial_smoothness', 1.0)
    )